{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://humanist.kdl.kcl.ac.uk/Archives/Current/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "response2 = requests.get(url2)\n",
    "soup = BeautifulSoup(response.text, features=\"html.parser\")\n",
    "soup2 = BeautifulSoup(response2.text, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_links = soup.find_all('a', href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1987-1988.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1988-1989.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1989-1990.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1990-1991.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1991-1992.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1992-1993.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1993-1994.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1994-1995.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1995-1996.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1996-1997.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1997-1998.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1998-1999.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.1999-2000.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.2000-2001.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.2001-2002.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.2002-2003.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.2003-2004.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.2004-2005.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.2005-2006.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.2006-2007.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/humanist.2007-2008.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_urls = []\n",
    "for link in volume_links[1:-1]:\n",
    "    full_url = link.get(\"href\")\n",
    "    full_urls.append(\"https://humanist.kdl.kcl.ac.uk/Archives/Converted_Text/\"+full_url)\n",
    "\n",
    "full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1987-1988.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1988-1989.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1989-1990.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1990-1991.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1991-1992.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1992-1993.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1993-1994.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1994-1995.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1995-1996.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1996-1997.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1997-1998.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1998-1999.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.1999-2000.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.2000-2001.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.2001-2002.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.2002-2003.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.2003-2004.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.2004-2005.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.2005-2006.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.2006-2007.txt',\n",
       " 'https://humanist.kdl.kcl.ac.uk/Archives/Current/humanist.2007-2008.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_urls2 = []\n",
    "for link in volume_links[1:-1]:\n",
    "    full_url2 = link.get(\"href\")\n",
    "    full_urls2.append(\"https://humanist.kdl.kcl.ac.uk/Archives/Current/\"+full_url2)\n",
    "\n",
    "full_urls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_url_coverted.csv\", 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"url\"])\n",
    "\n",
    "    for url in full_urls:\n",
    "        writer.writerow([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_url_current.csv\", 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"url2\"])\n",
    "\n",
    "    for url2 in full_urls2:\n",
    "        writer.writerow([url2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=pd.read_csv(\"cleaned_url_coverted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2=pd.read_csv(\"cleaned_url_current.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"converted_text.csv\", 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(\"url\")\n",
    "\n",
    "    for index,row in texts.iterrows():\n",
    "        url = row[\"url\"]\n",
    "        volume_response = requests.get(url)\n",
    "        volume_soup = BeautifulSoup(volume_response.text, \"html.parser\")\n",
    "        script = volume_soup.get_text()\n",
    "\n",
    "        first_10000_chars = script[:10000]\n",
    "\n",
    "        writer.writerow([row[\"url\"], first_10000_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"current_text.csv\", 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(\"url2\")\n",
    "\n",
    "    for index,row in texts2.iterrows():\n",
    "        url2 = row[\"url2\"]\n",
    "        volume_response2 = requests.get(url2)\n",
    "        volume_soup2 = BeautifulSoup(volume_response2.text, \"html.parser\")\n",
    "        script2 = volume_soup2.get_text()\n",
    "\n",
    "        first_10000_chars = script[:10000]\n",
    "\n",
    "        writer.writerow([row[\"url2\"], first_10000_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"converted_text.csv\")\n",
    "df2 = pd.read_csv(\"current_text.csv\")\n",
    "\n",
    "combined_df = pd.concat([df1,df2],ignore_index=True)\n",
    "combined_df.to_csv(\"web_scraped_humanist_listserv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
